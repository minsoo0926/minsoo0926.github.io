---
layout: default
---

<section>
    <pre>
  guest@minsoo.blog:~/projects$ cd PPO-scratch
  
  guest@minsoo.blog:~/projects/PPO-scratch$ cat README.md</pre>
    <h1>
        PPO-scratch
    </h1>
    <h3><a href="https://github.com/minsoo0926/PPO-scratch">Github link</a>&nbsp;&nbsp;&nbsp;&nbsp;</h3>
    <h2>Description</h2>
    <ul>
        <li>A complete implementation of the Proximal Policy Optimization (PPO) algorithm from scratch, compatible with OpenAI Gym/Gymnasium API.</li>
    </ul>
    <h2>Features</h2>
    <ul>
        <li><strong>Gymnasium Compatible:</strong> Compatible with any Gymnasium environment (discrete or continuous)</li>
        <li><strong>Training Utilities:</strong> Includes training scripts with progress tracking and visualization</li>
        <li><strong>Model Persistence:</strong> Save and load trained models with automatic compatibility checking</li>
    </ul>
    <h2>Tech Stack</h2>
    <ul>
        <li>ML framework: pytorch, gymnasium</li>
    </ul>
</section>
<pre>    guest@minsoo.blog:~/projects/PPO-scratch$ <a href="../">cd ..</a></pre>